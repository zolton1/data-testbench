import sys, os
sys.path.append(os.path.abspath(os.path.dirname(__file__)))

from airflow import DAG
from datetime import datetime
from airflow.decorators import task
from airflow.models import Variable
from airflow.datasets import Dataset
import hashlib
import string
import random
from typing import List
from datetime import timedelta

doc_md_DAG = """
### performance test DAG

to customise the DAG you can set the following optional Airflow variables:

- **performance_test_number_of_tasks:**

    It sets the amount of tasks generated by the script.

    default value: 10

- **performance_test_number_of_dags:**

    It sets the amount of DAGs generated by the script.

    default value: 20

- **performance_test_maximum_number_of_dataset_deps:**

    It sets the maximum number of Dataset dependencies between DAGs.

    default value: 5

- **performance_test_execution_timeout_seconds*:**

    It sets the task timeout(seconds) for all tasks.

    default value: 10

"""

default_args = {
    "owner": "airflow",
    "start_date": datetime(2024, 2, 22),
    "retries": 0,
}

var_number_of_tasks = int(
    Variable.get("performance_test_number_of_tasks", default_var=10)
)

var_number_of_dags = int(
    Variable.get("performance_test_number_of_dags", default_var=20)
)

var_maximum_number_of_dataset_deps = int(
    Variable.get("performance_test_maximum_number_of_dataset_deps", default_var=5)
)

var_execution_timeout = timedelta(
    seconds=int(
        Variable.get("performance_test_execution_timeout_seconds", default_var=10)
    )
)

var_str_length = int(
    Variable.get("performance_test_str_length", default_var=10000)
)

task_list = [f"task_{x}" for x in list(range(0, var_number_of_tasks))]

def _test_hash_string(str_length) -> None:
    m = hashlib.sha256()
    txt = "".join(random.choices(string.ascii_uppercase + string.digits, k=str_length))
    m.update(txt.encode("utf-8"))
    print(m.hexdigest())

def _gen_dag_name(iter):
    return f"performance_test_dag_{iter}"

def _gen_dataset_name(dag_id, task_id):
    return f"{dag_id}_dataset_{task_id}"

def _gen_dataset_deps(iter, task_list=task_list, maximum_number_of_dataset_deps=var_maximum_number_of_dataset_deps):
    return [Dataset(_gen_dataset_name(_gen_dag_name(iter), task_instance)) for task_instance in task_list[0::max(1,iter)]][:maximum_number_of_dataset_deps]


for i in range(var_number_of_dags):
    with DAG(
        f"{_gen_dag_name(iter=i)}",
        default_args=default_args,
        description="performance-test",
        schedule="10 * * * *" if i==0 else _gen_dataset_deps(iter=i-1, task_list=task_list),
        catchup=False,
        max_active_runs=1,
        doc_md=doc_md_DAG
    ) as dag:

        a = int(len(task_list) * 0.2)

        b = int(len(task_list) * 0.3)

        c = int(len(task_list) * 0.9)

        task_list_a = task_list[:a]

        task_list_b = task_list[a:b]

        task_list_c = task_list[b:c]

        task_list_d = task_list[c:]

        @task
        def pt(var_str_length=var_str_length):
            _test_hash_string(var_str_length)

        def _connect(a_list, b_list, dag=dag):
            for x in a_list:
                for y in b_list:
                    dag.get_task(x) >> dag.get_task(y)

        [
            pt.override(
                task_id=ti,
                retries=0,
                execution_timeout=var_execution_timeout,
                outlets=[Dataset(_gen_dataset_name(dag.dag_id, ti))],
            )()
            for ti in task_list
        ]

        _connect(a_list=task_list_a, b_list=task_list_b)

        _connect(a_list=task_list_a, b_list=task_list_c)

        _connect(a_list=task_list_b, b_list=task_list_c)

        _connect(a_list=task_list_b, b_list=task_list_d)

        _connect(a_list=task_list_c, b_list=task_list_d)
